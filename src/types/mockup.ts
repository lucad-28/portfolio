import { Project } from "./project";
import { Certificate } from "./certificate";

export const PROJECTS: Record<string, Project> = {
  spam_or_ham: {
    title: "Modelo Clasificador de Correo Spam o Ham",
    description:
      "Modelo de ML para clasificar correos electrónicos como spam utilizando técnicas de procesamiento de lenguaje natural.",
    fullDescription:
      "Desarrollé un modelo de clasificación de correos electrónicos que utiliza técnicas avanzadas de procesamiento de lenguaje natural (NLP) para identificar y clasificar correos como spam o ham. El modelo fue entrenado con un dataset de +5,000 correos etiquetados.",
    technologies: [
      "Python",
      "Scikit-learn",
      "Pandas",
      "NLTK (Natural Language Toolkit)",
      "Matplotlib",
      "Seaborn",
    ],
    image: "/thumbnails/soh.png",
    category: "Machine Learning",
    duration: "1 semana",
    team: "Proyecto individual",
    challenges: [
      "Palabras irrelevantes y ruido en los datos",
      "Desbalanceo de clases entre spam (60%) y ham (40%)",
      "Interpretabilidad del modelo",
    ],
    solutions: [
      "Usar ntlk para limpieza y tokenización de texto",
      "Usar SMOTE con oversampling para balancear clases",
      "Vectorizacion con sklearn.CountVectorizer",
    ],
    results: [
      "98% accuracy en el conjunto de prueba",
      "Prioridad en la detección de ham",
      "Modelo balanceado y robusto",
    ],
    demoUrl: "https://lupredict-web.vercel.app/",
    videoUrl:
      "https://github.com/user-attachments/assets/c7049885-d591-4a51-aa2f-79f5ee34d362",
    githubUrl: "https://github.com/lucad-28/soh",
  },
  tractchun: {
    title: "Dashboard de Seguimiento de Ventas de Tickets",
    description:
      "Dashboard interactivo en Power BI para el monitoreo y análisis de ventas de tickets con segmentaciones avanzadas y visualizaciones dinámicas",
    fullDescription:
      "Desarrollé un dashboard completo de seguimiento de ventas de tickets que permite analizar el desempeño comercial desde múltiples perspectivas. El sistema incluye visualizaciones interactivas para el histórico de tickets (2016-2020), métricas clave de rendimiento, y un sistema de ranking de agentes basado en múltiples criterios de evaluación. Basandome en el curso 'Microsoft Power BI – Curso de Power BI Desktop' parte de 'Inteligencia Artificial y Análisis de Datos con PowerBI' de Guayerd + IBM",
    technologies: [
      "Power BI Desktop",
      "DAX",
      "Power Query",
      "Bookmarks",
      "Segmentation",
    ],
    image: "/thumbnails/tractchun.png",
    category: "Business Intelligence & Analytics",
    duration: "2 meses",
    team: "Proyecto Individual",
    challenges: [
      "+2,000 registros de ventas",
      "Ranking de agentes con múltiples criterios",
      "Rendimiento del dashboard",
    ],
    solutions: [
      "Modelado estrella",
      "Multiples medidas RANKX combinadas",
      "Optimización de consultas DAX",
    ],
    results: [
      "Dashboard con navegación intuitiva",
      "Tiempo de carga optimizado (< 3 segundos)",
      "Sistema de ranking automatizado",
      "Interfaz dual (modo claro/oscuro)",
    ],
    demoUrl: "",
    videoUrl:
      "https://github.com/user-attachments/assets/9d5c6a47-e362-470f-8908-b3b66d027b8f",
    githubUrl: "https://github.com/lucad-28/tractchun-visualization-data",
  },
  etlas: {
    title: "Etlas - Asistente ETL Inteligente con IA",
    description:
      "Plataforma web que utiliza la API de OpenAI para generar código ETL personalizado mediante prompts especializados, con gestión de usuarios y almacenamiento de esquemas de bases de datos",
    fullDescription:
      "Desarrollé una aplicación web completa que funciona como asistente ETL inteligente, permitiendo a los usuarios generar código de extracción, transformación y carga de datos mediante inteligencia artificial. El sistema integra la API de OpenAI con prompts personalizados para configurar un asistente especializado en ETL, incluye autenticación mediante Google OAuth, y permite a cada usuario almacenar y gestionar sus mensajes, conversaciones y esquemas de bases de datos. La arquitectura incluye un backend dockerizado en Python con FastAPI desplegado en Google Cloud Run, base de datos PostgreSQL en Supabase, y frontend en Next.js hosteado en Vercel.",
    technologies: [
      "Next.js",
      "FastAPI",
      "Python",
      "PostgreSQL",
      "OpenAI API",
      "Docker",
      "Google Cloud Run",
      "Vercel",
      "Supabase",
      "NextAuth.js",
      "Google OAuth",
    ],
    image: "/thumbnails/etlas.png",
    category: "AI & Automatization",
    duration: "2 semanas",
    team: "3 analysts",
    challenges: [
      "Integración compleja con OpenAI API",
      "Sistema de autenticación seguro multi-plataforma",
      "Gestión de estado de conversaciones por usuario",
      "Optimización de prompts para ETL especializado",
      "Arquitectura distribuida con múltiples servicios",
    ],
    solutions: [
      "Prompt especializado para tareas ETL",
      "NextAuth.js con Google OAuth para autenticación seamless",
      "Base de datos relacional optimizada para mensajes y esquemas",
      "API RESTful con FastAPI y validación Pydantic",
      "Containerización con Docker para despliegue escalable",
    ],
    results: [
      "Generación automatizada de código ETL de alta calidad",
      "Sistema de usuarios con persistencia de conversaciones",
      "API backend escalable",
      "Interfaz de usuario intuitiva y responsive",
      "Tiempo de respuesta promedio < 2 segundos",
      "Gestión eficiente de esquemas de BD por usuario",
    ],
    demoUrl: "https://etlas.vercel.app",
    videoUrl:
      "https://github.com/user-attachments/assets/dd17b77f-eb3f-41ae-bbeb-15d5da691c48",
    githubUrl: "https://github.com/lucad-28/etlas-b",
    frontendGithubUrl: "https://github.com/lucad-28/etlas-f",
  },
  "ab-testing-framework": {
    title: "A/B Testing Framework",
    description:
      "Developed a statistical framework for A/B testing with automated significance testing and power analysis.",
    fullDescription:
      "Created a comprehensive A/B testing framework that automates the entire experimentation process from power analysis to result interpretation, ensuring statistical rigor and business relevance.",
    technologies: ["Python", "Scipy", "Statsmodels", "Matplotlib", "Streamlit"],
    image: "/placeholder.svg?height=400&width=800",
    category: "Experimentation",
    duration: "2 months",
    team: "Solo project",
    challenges: [
      "Multiple testing correction",
      "Sequential testing capabilities",
      "User-friendly interface for non-statisticians",
    ],
    solutions: [
      "Implemented Benjamini-Hochberg procedure",
      "Added sequential probability ratio tests",
      "Built Streamlit web interface",
    ],
    results: [
      "Reduced experiment analysis time by 70%",
      "Standardized testing across organization",
      "Prevented 5+ false positive decisions",
    ],
    demoUrl: "#",
    videoUrl: "https://www.youtube.com/embed/dQw4w9WgXcQ",
    githubUrl: "#",
  },
};

export const CERTIFICATES: Certificate[] = [
  {
    id: "aws-certified-data-analytics",
    title: "AWS Certified Data Analytics - Specialty",
    issuer: "Amazon Web Services",
    date: "March 2024",
    image: "/placeholder.svg?height=400&width=600",
    description:
      "Validates expertise in designing and implementing AWS data analytics solutions. Demonstrates deep understanding of AWS data analytics services and how to use them to derive insights from data.",
    skills: [
      "Amazon Redshift",
      "Amazon EMR",
      "AWS Glue",
      "Amazon Kinesis",
      "Amazon QuickSight",
      "AWS Lambda",
      "Data Lake Architecture",
      "ETL Processes",
    ],
    credentialId: "AWS-DAS-2024-001",
    validUntil: "March 2027",
  },
  {
    id: "google-professional-data-engineer",
    title: "Google Professional Data Engineer",
    issuer: "Google Cloud",
    date: "January 2024",
    image: "/placeholder.svg?height=400&width=600",
    description:
      "Demonstrates ability to design, build, operationalize, secure, and monitor data processing systems with a particular emphasis on security and compliance, scalability and efficiency, reliability and fidelity, and flexibility and portability.",
    skills: [
      "BigQuery",
      "Cloud Dataflow",
      "Cloud Pub/Sub",
      "Cloud Storage",
      "Data Studio",
      "Machine Learning",
      "Data Pipeline Design",
      "Data Governance",
    ],
    credentialId: "GCP-PDE-2024-001",
    validUntil: "January 2026",
  },
  {
    id: "microsoft-azure-data-scientist",
    title: "Microsoft Certified: Azure Data Scientist Associate",
    issuer: "Microsoft",
    date: "November 2023",
    image: "/placeholder.svg?height=400&width=600",
    description:
      "Validates skills in applying data science and machine learning to implement and run machine learning workloads on Azure using Azure Machine Learning Service.",
    skills: [
      "Azure Machine Learning",
      "Python",
      "Jupyter Notebooks",
      "MLOps",
      "Model Deployment",
      "Feature Engineering",
      "Model Evaluation",
      "Automated ML",
    ],
    credentialId: "MS-AZ-DS-2023-001",
    validUntil: "November 2025",
  },
  {
    id: "tableau-certified-data-analyst",
    title: "Tableau Certified Data Analyst",
    issuer: "Tableau",
    date: "September 2023",
    image: "/placeholder.svg?height=400&width=600",
    description:
      "Validates skills in connecting to and transforming data, exploring and analyzing data, and sharing insights through data visualizations and dashboards using Tableau.",
    skills: [
      "Data Visualization",
      "Dashboard Design",
      "Calculated Fields",
      "Table Calculations",
      "Data Blending",
      "Parameters",
      "Actions",
      "Performance Optimization",
    ],
    credentialId: "TCA-2023-001",
    validUntil: "September 2026",
  },
  {
    id: "databricks-certified-associate",
    title: "Databricks Certified Associate Developer",
    issuer: "Databricks",
    date: "July 2023",
    image: "/placeholder.svg?height=400&width=600",
    description:
      "Demonstrates foundational knowledge of Apache Spark and the ability to perform basic data engineering and data science tasks using Databricks.",
    skills: [
      "Apache Spark",
      "PySpark",
      "Delta Lake",
      "MLflow",
      "Databricks Notebooks",
      "Data Processing",
      "Spark SQL",
      "Distributed Computing",
    ],
    credentialId: "DB-CAD-2023-001",
    validUntil: "July 2025",
  },
  {
    id: "python-institute-pcap",
    title: "PCAP – Certified Associate in Python Programming",
    issuer: "Python Institute",
    date: "May 2023",
    image: "/placeholder.svg?height=400&width=600",
    description:
      "Validates fundamental programming skills in Python and the ability to accomplish coding tasks related to the basics of programming in the Python language.",
    skills: [
      "Python Programming",
      "Object-Oriented Programming",
      "Data Structures",
      "Exception Handling",
      "File Operations",
      "Modules and Packages",
      "Standard Library",
      "Code Debugging",
    ],
    credentialId: "PCAP-2023-001",
    validUntil: "Lifetime",
  },
];
